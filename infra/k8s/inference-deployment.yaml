apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-deployment
  namespace: vision-platform
  labels:
    app: inference
    component: cv-model
spec:
  replicas: 2
  selector:
    matchLabels:
      app: inference
  template:
    metadata:
      labels:
        app: inference
        component: cv-model
    spec:
      containers:
      - name: inference
        image: vision-platform/inference:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 50051
          name: grpc
        env:
        - name: PORT
          value: "50051"
        envFrom:
        - configMapRef:
            name: vision-platform-config
        resources:
          requests:
            memory: "1Gi"
            cpu: "1000m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        # Uncomment for GPU support
        # resources:
        #   limits:
        #     nvidia.com/gpu: 1
      restartPolicy: Always
      # Uncomment for GPU node selection
      # nodeSelector:
      #   accelerator: nvidia-tesla-t4

