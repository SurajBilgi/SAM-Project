# Real-Time Vision Platform

A production-grade, cloud-native real-time computer vision platform with ultra-low latency video streaming and inference.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     WebRTC/WS      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚   Frontend   â”‚
â”‚  (Webcam)   â”‚                     â”‚  (React/TS)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     RTSP/HTTP       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IP Cameras  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚  API Service â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚   (FastAPI)  â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Streaming   â”‚
                                    â”‚   Service    â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Inference   â”‚
                                    â”‚  Service     â”‚
                                    â”‚  (gRPC/CV)   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

- **Ultra-Low Latency**: Sub-200ms end-to-end latency
- **Multiple Input Sources**: Webcam + IP Cameras (RTSP/HTTP)
- **Real-Time Inference**: Live computer vision processing
- **Production Ready**: Docker + Kubernetes ready
- **Auto-Recovery**: Automatic reconnection and graceful degradation
- **Scalable**: Horizontal pod autoscaling support
- **Observable**: Structured logging and metrics

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Node.js 18+ (for local frontend development)
- Python 3.10+ (for local backend development)
- Kubernetes cluster (for production deployment)

### Local Development with Docker Compose

```bash
# Build and start all services
docker-compose up --build

# Access the application
# Frontend: http://localhost:3000
# API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

### Local Development without Docker

#### Backend Services

```bash
# API Service
cd backend/api
pip install -r requirements.txt
uvicorn main:app --reload --port 8000

# Streaming Service
cd backend/streaming
pip install -r requirements.txt
python main.py

# Inference Service
cd backend/inference
pip install -r requirements.txt
python server.py
```

#### Frontend

```bash
cd frontend
npm install
npm run dev
```

## ğŸ“¦ Production Deployment (Kubernetes)

```bash
# Apply all Kubernetes manifests
kubectl apply -f infra/k8s/namespace.yaml
kubectl apply -f infra/k8s/

# Verify deployment
kubectl get pods -n vision-platform
kubectl get services -n vision-platform

# Access via LoadBalancer
kubectl get svc frontend-service -n vision-platform
```

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ frontend/                 # TypeScript React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/      # React components
â”‚   â”‚   â”œâ”€â”€ services/        # API and WebRTC services
â”‚   â”‚   â””â”€â”€ main.tsx
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/                 # FastAPI control plane
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚
â”‚   â”œâ”€â”€ streaming/           # Frame ingestion service
â”‚   â”‚   â”œâ”€â”€ stream_manager.py
â”‚   â”‚   â”œâ”€â”€ rtsp_reader.py
â”‚   â”‚   â”œâ”€â”€ frame_bus.py
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/           # CV inference service
â”‚   â”‚   â”œâ”€â”€ server.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ protos/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚
â”‚   â””â”€â”€ common/              # Shared utilities
â”‚       â”œâ”€â”€ models.py
â”‚       â””â”€â”€ utils.py
â”‚
â””â”€â”€ infra/
    â”œâ”€â”€ docker/              # Docker configurations
    â””â”€â”€ k8s/                 # Kubernetes manifests
        â”œâ”€â”€ namespace.yaml
        â”œâ”€â”€ *-deployment.yaml
        â”œâ”€â”€ *-service.yaml
        â””â”€â”€ hpa.yaml
```

## ğŸ”§ Configuration

### Environment Variables

#### API Service
- `INFERENCE_SERVICE_URL`: gRPC endpoint for inference service
- `STREAMING_SERVICE_URL`: HTTP endpoint for streaming service
- `LOG_LEVEL`: Logging level (DEBUG, INFO, WARNING, ERROR)

#### Streaming Service
- `INFERENCE_SERVICE_URL`: gRPC endpoint for inference service
- `MAX_FPS`: Maximum frames per second to process
- `LOG_LEVEL`: Logging level

#### Inference Service
- `MODEL_VERSION`: Version identifier for the model
- `DEVICE`: Compute device (cpu, cuda)
- `LOG_LEVEL`: Logging level

#### Frontend
- `VITE_API_URL`: Backend API base URL
- `VITE_WS_URL`: WebSocket URL for real-time communication

## ğŸ“Š Performance Characteristics

- **Target Latency**: < 200ms end-to-end
- **Frame Rate**: Up to 30 FPS
- **Backpressure Handling**: Drops frames instead of buffering
- **Auto-Reconnect**: Automatic recovery on connection loss

## ğŸ”’ Security

- Token-based session management
- No hardcoded credentials
- Environment-based configuration
- Secure RTSP credential handling
- CORS configuration for production

## ğŸ“ˆ Observability

- Structured JSON logging
- Health check endpoints on all services
- Metrics for FPS, latency, and throughput
- Ready for Prometheus/Grafana integration

## ğŸ› ï¸ Development

### Adding a New CV Model

1. Update `backend/inference/model.py`
2. Implement the `InferenceModel` interface
3. Update model loading logic in `server.py`
4. Rebuild inference service Docker image

### Adding a New Camera Source

1. Extend `backend/streaming/stream_manager.py`
2. Add new reader class (similar to `RTSPReader`)
3. Register in camera routes

## ğŸ§ª Testing

```bash
# Backend tests
cd backend/api
pytest

# Frontend tests
cd frontend
npm test
```

## ğŸ“ API Documentation

Once running, visit:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## ğŸ¤ Contributing

This is a production-grade foundation designed for extensibility. Key extension points:

- Custom CV models in `backend/inference/`
- Additional camera sources in `backend/streaming/`
- Frontend visualization components in `frontend/src/components/`

## ğŸ“„ License

See LICENSE file for details.

## ğŸ”® Future Enhancements

- [ ] LLM-based reasoning integration
- [ ] Multi-model ensemble support
- [ ] Advanced analytics and alerting
- [ ] Cloud storage integration
- [ ] Mobile app support
